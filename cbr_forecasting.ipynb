{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "from utils.data import *\n",
    "\n",
    "from catboost import CatBoostRegressor, Pool\n",
    "from sklearn.metrics import mean_absolute_error as MAE, mean_absolute_percentage_error as MAPE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path = \"data/huge_stock_market_dataset/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "lags = 32\n",
    "horizon = 8\n",
    "stride = 1\n",
    "val_size = 0.15\n",
    "test_size = 0.0\n",
    "features = 1\n",
    "\n",
    "model_params = {\"silent\": True, \"random_seed\": 13, 'loss_function': 'MultiRMSE',  'eval_metric': 'MultiRMSE', \"iterations\": 100}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24it [01:21,  3.39s/it]\n"
     ]
    }
   ],
   "source": [
    "ts_iterator = get_hsm_dataset(dataset_path, selected_files=f\"{dataset_path}/selected.csv\")\n",
    "\n",
    "results = []\n",
    "for time_series in tqdm(ts_iterator):\n",
    "    (X_train, y_train), (X_val, y_val), _, X_scaler, y_scaler = create_ts(time_series[[\"Close\"]], time_series[\"Close\"], lags=lags, horizon=horizon, stride=stride,\\\n",
    "                                            data_preprocess=\"log_returns\",\\\n",
    "                                            val_size=val_size, test_size=test_size)\n",
    "    train_dl = Pool(X_train, label=y_train)\n",
    "    val_dl = Pool(X_val, label=y_val)\n",
    "    \n",
    "    model = CatBoostRegressor(**model_params)\n",
    "    # model.fit(train_dl, eval_set=val_dl, early_stopping_rounds=5, use_best_model=True)\n",
    "    model.fit(train_dl)\n",
    "    results.append({\"train\": MAE(y_train, model.predict(X_train)), \"val\": MAE(y_val, model.predict(X_val))})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'train': 0.0988395057316753, 'val': 0.08789940327982101},\n",
       " {'train': 0.036776620788563044, 'val': 0.05224397927529138},\n",
       " {'train': 0.006398688442784712, 'val': 0.005172470906840296},\n",
       " {'train': 0.009092196664038262, 'val': 0.010550395835513177},\n",
       " {'train': 0.014213012797644711, 'val': 0.01296672307139793},\n",
       " {'train': 0.026789991070434185, 'val': 0.027064470333851093},\n",
       " {'train': 0.021529949709914814, 'val': 0.03407264577985521},\n",
       " {'train': 0.01658921208340948, 'val': 0.015108352877514712},\n",
       " {'train': 0.008285412407542502, 'val': 0.006250136940208484},\n",
       " {'train': 0.007517652381158116, 'val': 0.006931711833014095},\n",
       " {'train': 0.050254284523359116, 'val': 0.04007008330234592},\n",
       " {'train': 0.03686151096930589, 'val': 0.020557156263218287},\n",
       " {'train': 0.033912738350219254, 'val': 0.03420939772485955},\n",
       " {'train': 0.01133497802904989, 'val': 0.006658268647246142},\n",
       " {'train': 0.0178769694550136, 'val': 0.010439005923165557},\n",
       " {'train': 0.04145062412181437, 'val': 0.026834370596076018},\n",
       " {'train': 0.02826567476326579, 'val': 0.02181821279703807},\n",
       " {'train': 0.006472212454582445, 'val': 0.0065344038917009885},\n",
       " {'train': 0.017499860731006088, 'val': 0.0169129753132065},\n",
       " {'train': 0.008870150513868528, 'val': 0.005629660013642474},\n",
       " {'train': 0.014913038508258685, 'val': 0.0072512830202717425},\n",
       " {'train': 0.008635722232962051, 'val': 0.00695486310403283},\n",
       " {'train': 0.022910295998689923, 'val': 0.02846124111166505},\n",
       " {'train': 0.01792677868037782, 'val': 0.008923770234357883}]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame([{key: value for key, value in x.items()} for x in results]).to_csv(\"results\\\\pure_cbr.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Augmentation with QuantGAN synthetic data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "synthetic_path = f\"{dataset_path}synthetic/QuantGAN/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24it [05:34, 13.93s/it]\n"
     ]
    }
   ],
   "source": [
    "ts_iterator = get_hsm_dataset(dataset_path, selected_files=f\"{dataset_path}/selected.csv\")\n",
    "\n",
    "results = []\n",
    "for ts_index, time_series in tqdm(enumerate(ts_iterator)):\n",
    "    (X_train, y_train), (X_val, y_val), _, X_scaler, y_scaler = create_ts(time_series[[\"Close\"]], time_series[\"Close\"], lags=lags, horizon=horizon, stride=stride,\\\n",
    "                                            data_preprocess=\"log_returns\", val_size=val_size, test_size=test_size)\n",
    "\n",
    "    synth_time_series = np.load(f\"{synthetic_path}selected{ts_index}.npy\")\n",
    "    X_synth, y_synth = [], []\n",
    "    for i in range(synth_time_series.shape[0]):\n",
    "        (X, y), *_ = create_ts(synth_time_series[i].reshape(127, 1), synth_time_series[i], lags=lags, horizon=horizon, stride=stride,\\\n",
    "                                            data_preprocess=None, val_size=0, test_size=0)\n",
    "        X_synth.append(X)\n",
    "        y_synth.append(y)\n",
    "\n",
    "    # using train and synth data\n",
    "    # X_train = np.row_stack((X_train, *X_synth))\n",
    "    # y_train = np.row_stack((y_train, *y_synth))\n",
    "    # using only synth data\n",
    "    X_synth = np.row_stack(X_synth)\n",
    "    y_synth = np.row_stack(y_synth)\n",
    "    \n",
    "    model = CatBoostRegressor(**model_params)\n",
    "    model.fit(X_synth, y_synth)\n",
    "    results.append({\"train\": MAE(y_train, model.predict(X_train)), \"val\": MAE(y_val, model.predict(X_val))})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'train': 0.11364971720278666, 'val': 0.08693882978348716},\n",
       " {'train': 0.03939550428315655, 'val': 0.052459493981945375},\n",
       " {'train': 0.00732770814826462, 'val': 0.005236142415456854},\n",
       " {'train': 0.01059379613775083, 'val': 0.010737764634326963},\n",
       " {'train': 0.01697277804361385, 'val': 0.013095494929244653},\n",
       " {'train': 0.028265612522638605, 'val': 0.027197195484945588},\n",
       " {'train': 0.023163520709057094, 'val': 0.03428551024400679},\n",
       " {'train': 0.01781297319001693, 'val': 0.015009823889852038},\n",
       " {'train': 0.008815946612340957, 'val': 0.006289377438780666},\n",
       " {'train': 0.008114468512206324, 'val': 0.006912829730923024},\n",
       " {'train': 0.0523987422383527, 'val': 0.04006941343734814},\n",
       " {'train': 0.0387281236950867, 'val': 0.020793728892764277},\n",
       " {'train': 0.035364503305783196, 'val': 0.034271758451891676},\n",
       " {'train': 0.01209907797697047, 'val': 0.006735574436137125},\n",
       " {'train': 0.018854615035617854, 'val': 0.010742046181860518},\n",
       " {'train': 0.04310588466219803, 'val': 0.026617328995026508},\n",
       " {'train': 0.029348125179202933, 'val': 0.02198877216908514},\n",
       " {'train': 0.006744272259911725, 'val': 0.006567965933592235},\n",
       " {'train': 0.018174952869972675, 'val': 0.016780686864665422},\n",
       " {'train': 0.00932247481632744, 'val': 0.005683825890755416},\n",
       " {'train': 0.014830327588825108, 'val': 0.007292659898572823},\n",
       " {'train': 0.008724990459025006, 'val': 0.006987462863657011},\n",
       " {'train': 0.02341670086206556, 'val': 0.028488094829576996},\n",
       " {'train': 0.018824907636839926, 'val': 0.008950937460829603}]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(results).to_csv(\"results\\\\QuantGAN_synth_cbr.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Augmentation with FourierFlow synthetic data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "synthetic_path = f\"{dataset_path}synthetic/FourierFlow/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24it [06:17, 15.74s/it]\n"
     ]
    }
   ],
   "source": [
    "ts_iterator = get_hsm_dataset(dataset_path, selected_files=f\"{dataset_path}/selected.csv\")\n",
    "\n",
    "results = []\n",
    "for ts_index, time_series in tqdm(enumerate(ts_iterator)):\n",
    "    (X_train, y_train), (X_val, y_val), _, X_scaler, y_scaler = create_ts(time_series[[\"Close\"]], time_series[\"Close\"], lags=lags, horizon=horizon, stride=stride,\\\n",
    "                                            data_preprocess=\"log_returns\", val_size=val_size, test_size=test_size)\n",
    "\n",
    "    synth_time_series = np.load(f\"{synthetic_path}selected{ts_index}.npy\")\n",
    "    X_synth, y_synth = [], []\n",
    "    for i in range(synth_time_series.shape[0]):\n",
    "        (X, y), *_ = create_ts(synth_time_series[i].reshape(- 1, 1), synth_time_series[i], lags=lags, horizon=horizon, stride=stride,\\\n",
    "                                            data_preprocess=None, val_size=0, test_size=0)\n",
    "        X_synth.append(X)\n",
    "        y_synth.append(y)\n",
    "\n",
    "    # using only synth data\n",
    "    X_synth = np.row_stack(X_synth)\n",
    "    y_synth = np.row_stack(y_synth)\n",
    "    \n",
    "    model = CatBoostRegressor(**model_params)\n",
    "    model.fit(X_synth, y_synth)\n",
    "    results.append({\"train\": MAE(y_train, model.predict(X_train)), \"val\": MAE(y_val, model.predict(X_val))})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'train': 0.09882969821406513, 'val': 0.0869540823318531},\n",
       " {'train': 0.036082272080638675, 'val': 0.05232444090073629},\n",
       " {'train': 0.0069615580240817935, 'val': 0.005415019845985192},\n",
       " {'train': 0.009966313767670282, 'val': 0.010744250155907506},\n",
       " {'train': 0.015545456368400639, 'val': 0.013214770001767046},\n",
       " {'train': 0.027761418615884405, 'val': 0.027815877333321228},\n",
       " {'train': 0.02216573265505383, 'val': 0.03488480704845248},\n",
       " {'train': 0.017073057638110734, 'val': 0.015265211147737644},\n",
       " {'train': 0.00856632657447557, 'val': 0.006267763016168115},\n",
       " {'train': 0.007972571744596967, 'val': 0.007025755524765736},\n",
       " {'train': 0.051125795879236505, 'val': 0.04024711798893335},\n",
       " {'train': 0.03652465204079635, 'val': 0.02057526138288386},\n",
       " {'train': 0.033960769710549746, 'val': 0.0342125575489541},\n",
       " {'train': 0.011202240334118891, 'val': 0.006649479377551289},\n",
       " {'train': 0.017745643086933702, 'val': 0.010436071183911745},\n",
       " {'train': 0.04135179630390444, 'val': 0.0267602816160294},\n",
       " {'train': 0.028348878331417027, 'val': 0.021828671623061728},\n",
       " {'train': 0.006398695753643218, 'val': 0.00652910511177758},\n",
       " {'train': 0.017708218420252317, 'val': 0.017059651941048887},\n",
       " {'train': 0.00906007994409175, 'val': 0.0057663043110980795},\n",
       " {'train': 0.014972633228020091, 'val': 0.007253978904747329},\n",
       " {'train': 0.008748419506774564, 'val': 0.0069719171479989735},\n",
       " {'train': 0.022904491568402928, 'val': 0.028424948649290845},\n",
       " {'train': 0.017380583724700867, 'val': 0.008894050454880835}]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(results).to_csv(\"results\\\\FourierFlow_synth_cbr.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Augmentation with RealNVP synthetic data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24it [05:40, 14.17s/it]\n"
     ]
    }
   ],
   "source": [
    "synthetic_path = f\"{dataset_path}synthetic/RealNVP/\"\n",
    "ts_iterator = get_hsm_dataset(dataset_path, selected_files=f\"{dataset_path}/selected.csv\")\n",
    "\n",
    "results = []\n",
    "for ts_index, time_series in tqdm(enumerate(ts_iterator)):\n",
    "    (X_train, y_train), (X_val, y_val), _, X_scaler, y_scaler = create_ts(time_series[[\"Close\"]], time_series[\"Close\"], lags=lags, horizon=horizon, stride=stride,\\\n",
    "                                            data_preprocess=\"log_returns\", val_size=val_size, test_size=test_size)\n",
    "\n",
    "    synth_time_series = np.load(f\"{synthetic_path}selected{ts_index}.npy\")\n",
    "    X_synth, y_synth = [], []\n",
    "    for i in range(synth_time_series.shape[0]):\n",
    "        (X, y), *_ = create_ts(synth_time_series[i].reshape(- 1, 1), synth_time_series[i], lags=lags, horizon=horizon, stride=stride,\\\n",
    "                                            data_preprocess=None, val_size=0, test_size=0)\n",
    "        X_synth.append(X)\n",
    "        y_synth.append(y)\n",
    "\n",
    "    # using only synth data\n",
    "    X_synth = np.row_stack(X_synth)\n",
    "    y_synth = np.row_stack(y_synth)\n",
    "    \n",
    "    model = CatBoostRegressor(**model_params)\n",
    "    model.fit(X_synth, y_synth)\n",
    "    results.append({\"train\": MAE(y_train, model.predict(X_train)), \"val\": MAE(y_val, model.predict(X_val))})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'train': 0.09004008455095977, 'val': 0.08946741592775331},\n",
       " {'train': 0.03521337150260906, 'val': 0.05330768129155191},\n",
       " {'train': 0.006063771878002708, 'val': 0.0051601933672822675},\n",
       " {'train': 0.008296476321771322, 'val': 0.010697864670940528},\n",
       " {'train': 0.012744381585464456, 'val': 0.013140320179621655},\n",
       " {'train': 0.02625484031529226, 'val': 0.027048461794527377},\n",
       " {'train': 0.020962111402496104, 'val': 0.034314910791500786},\n",
       " {'train': 0.016035150535266298, 'val': 0.015192270819212832},\n",
       " {'train': 0.00811985983992624, 'val': 0.006248309402716151},\n",
       " {'train': 0.007371306526956318, 'val': 0.006922752602291816},\n",
       " {'train': 0.049608531531847286, 'val': 0.04005521935692396},\n",
       " {'train': 0.03606473508858668, 'val': 0.020853320754902857},\n",
       " {'train': 0.03370531754944335, 'val': 0.03444359708007573},\n",
       " {'train': 0.011071202660555775, 'val': 0.00666058267394762},\n",
       " {'train': 0.017422976831430216, 'val': 0.010470713870071853},\n",
       " {'train': 0.040662411472381005, 'val': 0.026817191450552245},\n",
       " {'train': 0.027789374336607248, 'val': 0.02184326211752196},\n",
       " {'train': 0.0062890376896506785, 'val': 0.0065500883598391705},\n",
       " {'train': 0.01740619123782674, 'val': 0.016908922556456804},\n",
       " {'train': 0.008619320276270444, 'val': 0.005637694091380245},\n",
       " {'train': 0.014888532997790936, 'val': 0.007252282663606807},\n",
       " {'train': 0.00863448703903308, 'val': 0.006954981767170354},\n",
       " {'train': 0.023032347659791095, 'val': 0.028472320355797696},\n",
       " {'train': 0.017264633217785964, 'val': 0.008917644274293476}]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(results).to_csv(\"results\\\\RealNVP_synth_cbr.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.9 64-bit (system)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "1289e797c8b2364a1b561fc46768e8fcf8446b2e18e77ab0795c8743ff6ac10a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
