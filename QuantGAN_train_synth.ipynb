{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "\n",
    "from utils.data import get_hsm_dataset, get_solar_energy_dataset, get_fuel_prices_dataset, get_passengers_dataset, split_data\n",
    "from utils.metrics import MAPE, WAPE, MAE\n",
    "from utils.dl import QuantGAN_Discriminator, QuantGAN_Generator\n",
    "from utils.QuantGAN_gaussianize import Gaussianize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "hsm_dataset_path = \"data/huge_stock_market_dataset/\"\n",
    "solar_energy_dataset_path = \"data/solar_energy/\"\n",
    "fuel_prices_dataset_path = \"data/fuel_prices/\"\n",
    "passengers_dataset_path = \"data/air_passengers/\"\n",
    "models_dir = \"models/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)\n",
    "\n",
    "batches_to_gen = 10\n",
    "\n",
    "num_epochs = 10\n",
    "nz = 3\n",
    "batch_size = 80\n",
    "seq_len = 127\n",
    "clip = 0.01\n",
    "lr = 0.0002\n",
    "receptive_field_size = 127"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Loader32(torch.utils.data.Dataset):\n",
    "    def __init__(self, data, length):\n",
    "        assert len(data) >= length\n",
    "        self.data = data\n",
    "        self.length = length\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return torch.tensor(self.data[idx:idx+self.length]).reshape(- 1, self.length).to(torch.float32)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return max(len(self.data)-self.length, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_routine(time_series, ts_index, synthetic_path):\n",
    "    global val_size, test_size, batches_to_gen, num_epochs, nz, batch_size, seq_len, clip, lr, receptive_field_size\n",
    "    \n",
    "    # preprocessing steps according to the QuanGAN paper\n",
    "    df = time_series\n",
    "    returns = df.shift(1) / df - 1\n",
    "    log_returns = np.log(df / df.shift(1))[1:].to_numpy().reshape(- 1, 1)\n",
    "    standardScaler1 = StandardScaler()\n",
    "    standardScaler2 = StandardScaler()\n",
    "    gaussianize = Gaussianize()\n",
    "    log_returns_preprocessed = standardScaler2.fit_transform(gaussianize.fit_transform(standardScaler1.fit_transform(log_returns)))\n",
    "    data_size = log_returns.shape[0]\n",
    "\n",
    "    # defining models and optimizers\n",
    "    generator = QuantGAN_Generator().to(device)\n",
    "    discriminator = QuantGAN_Discriminator(seq_len).to(device)\n",
    "    disc_optimizer = torch.optim.RMSprop(discriminator.parameters(), lr=lr)\n",
    "    gen_optimizer = torch.optim.RMSprop(generator.parameters(), lr=lr)\n",
    "\n",
    "    # data preparing\n",
    "    dataset = Loader32(log_returns_preprocessed, 127)\n",
    "    dataloader = torch.utils.data.DataLoader(dataset, batch_size=batch_size)\n",
    "    \n",
    "    t = tqdm(range(num_epochs))\n",
    "    for epoch in t:\n",
    "        for idx, data in enumerate(dataloader, 0):\n",
    "\n",
    "            discriminator.zero_grad()\n",
    "            real = data.to(device)\n",
    "            batch_size, seq_len = real.size(0), real.size(2)\n",
    "            noise = torch.randn(batch_size, nz, seq_len, device=device)\n",
    "            fake = generator(noise).detach()\n",
    "            disc_loss = - torch.mean(discriminator(real)) + torch.mean(discriminator(fake))\n",
    "            disc_loss.backward()\n",
    "            disc_optimizer.step()\n",
    "\n",
    "            for dp in discriminator.parameters():\n",
    "                dp.data.clamp_(-clip, clip)\n",
    "    \n",
    "            if idx % 5 == 0:\n",
    "                generator.zero_grad()\n",
    "                gen_loss = - torch.mean(discriminator(generator(noise)))\n",
    "                gen_loss.backward()\n",
    "                gen_optimizer.step()\n",
    "        t.set_description('Discriminator Loss: %.8f Generator Loss: %.8f' % (disc_loss.item(), gen_loss.item()))\n",
    "    # saving model\n",
    "    torch.save(generator, models_dir +  f'QuantGAN_generator_selected{ts_index}.pth')\n",
    "\n",
    "    # generation synthetic time series\n",
    "    generator.eval()\n",
    "    ys = []\n",
    "    for _ in range(batches_to_gen):\n",
    "        with torch.no_grad():\n",
    "            noise = torch.randn(80, 3, 127).to(device)\n",
    "            y = generator(noise).cpu().detach().squeeze()\n",
    "\n",
    "        y = (y - y.mean(axis=0)) / y.std(axis=0)\n",
    "        y = standardScaler2.inverse_transform(y)\n",
    "        y = np.array([gaussianize.inverse_transform(np.expand_dims(x, 1)) for x in y]).squeeze()\n",
    "        y = standardScaler1.inverse_transform(y)\n",
    "\n",
    "        # some basic filtering to redue the tendency of GAN to produce extreme returns\n",
    "        y = y[(y.max(axis=1) <= 2 * log_returns.max()) & (y.min(axis=1) >= 2 * log_returns.min())]\n",
    "        y -= y.mean()\n",
    "        ys.append(y)\n",
    "\n",
    "    np.save(synthetic_path + f\"selected{ts_index}.npy\", np.row_stack(ys))\n",
    "\n",
    "    del discriminator, generator, disc_loss, gen_loss, dataloader, dataset, y\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "ts_iterator = get_hsm_dataset(hsm_dataset_path, selected_files=f\"{hsm_dataset_path}/selected100.csv\")\n",
    "synthetic_path = f\"{hsm_dataset_path}synthetic/QuantGAN/\"\n",
    "start_point = 100\n",
    "for _ in range(start_point): next(ts_iterator)\n",
    "\n",
    "for ts_index, time_series in enumerate(ts_iterator, start_point):\n",
    "    print(f\"Time Series #{ts_index}\")\n",
    "    model_routine(time_series, ts_index, synthetic_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 4\n",
    "ts_iterator = get_solar_energy_dataset(solar_energy_dataset_path, max_results=10)\n",
    "synthetic_path = f\"{solar_energy_dataset_path}synthetic/QuantGAN/\"\n",
    "start_point = 10\n",
    "for _ in range(start_point): next(ts_iterator)\n",
    "\n",
    "for ts_index, time_series in enumerate(ts_iterator, start_point):\n",
    "    print(f\"Time Series #{ts_index}\")\n",
    "    model_routine(time_series + 1e-9, ts_index, synthetic_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time Series #0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Discriminator Loss: 0.00000244 Generator Loss: -0.49736866: 100%|██████████| 10/10 [01:45<00:00, 10.57s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time Series #1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Discriminator Loss: 0.00000289 Generator Loss: -0.49748641: 100%|██████████| 10/10 [01:43<00:00, 10.36s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time Series #2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Discriminator Loss: 0.00000656 Generator Loss: -0.49795493: 100%|██████████| 10/10 [01:06<00:00,  6.68s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time Series #3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Discriminator Loss: -0.00005329 Generator Loss: -0.49990851: 100%|██████████| 10/10 [03:09<00:00, 18.93s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time Series #4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Discriminator Loss: 0.00001609 Generator Loss: -0.50114095: 100%|██████████| 10/10 [03:08<00:00, 18.83s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time Series #5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Discriminator Loss: -0.00001302 Generator Loss: -0.49993891: 100%|██████████| 10/10 [03:08<00:00, 18.81s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time Series #6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Discriminator Loss: -0.00000763 Generator Loss: -0.49995321: 100%|██████████| 10/10 [03:28<00:00, 20.84s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time Series #7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Discriminator Loss: 0.00000131 Generator Loss: -0.50278407: 100%|██████████| 10/10 [05:16<00:00, 31.69s/it]\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 10\n",
    "ts_iterator = get_fuel_prices_dataset(fuel_prices_dataset_path)\n",
    "synthetic_path = f\"{fuel_prices_dataset_path}synthetic/QuantGAN/\"\n",
    "start_point = 0\n",
    "for _ in range(start_point): next(ts_iterator)\n",
    "\n",
    "for ts_index, time_series in enumerate(ts_iterator, start_point):\n",
    "    print(f\"Time Series #{ts_index}\")\n",
    "    model_routine(time_series, ts_index, synthetic_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time Series #85\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Владислав\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\scipy\\stats\\_stats_py.py:1242: RuntimeWarning: underflow encountered in square\n",
      "  s = s**2\n",
      "c:\\Users\\Владислав\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\scipy\\stats\\_stats_py.py:1467: RuntimeWarning: underflow encountered in square\n",
      "  nval = 1.0/(n-2)/(n-3) * ((n**2-1.0)*m4/m2**2.0 - 3*(n-1)**2.0)\n",
      "c:\\Users\\Владислав\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\scipy\\stats\\_stats_py.py:1467: RuntimeWarning: invalid value encountered in divide\n",
      "  nval = 1.0/(n-2)/(n-3) * ((n**2-1.0)*m4/m2**2.0 - 3*(n-1)**2.0)\n",
      "c:\\Users\\Владислав\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\numpy\\core\\_methods.py:190: RuntimeWarning: underflow encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "c:\\Users\\Владислав\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\scipy\\stats\\_stats_py.py:1242: RuntimeWarning: overflow encountered in square\n",
      "  s = s**2\n",
      "c:\\Users\\Владислав\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\scipy\\stats\\_stats_py.py:1467: RuntimeWarning: overflow encountered in square\n",
      "  nval = 1.0/(n-2)/(n-3) * ((n**2-1.0)*m4/m2**2.0 - 3*(n-1)**2.0)\n",
      "c:\\Users\\Владислав\\Учеба\\Магистратура\\3 семестр\\synthetic data generation\\project\\utils\\QuantGAN_gaussianize.py:139: RuntimeWarning: overflow encountered in multiply\n",
      "  return np.sign(z) * np.sqrt(np.real(special.lambertw(delta * z ** 2)) / delta)\n",
      "c:\\Users\\Владислав\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\numpy\\core\\_methods.py:180: RuntimeWarning: invalid value encountered in reduce\n",
      "  ret = umr_sum(arr, axis, dtype, out, keepdims, where=where)\n",
      "c:\\Users\\Владислав\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\numpy\\core\\_methods.py:213: RuntimeWarning: invalid value encountered in reduce\n",
      "  arrmean = umr_sum(arr, axis, dtype, keepdims=True, where=where)\n",
      "c:\\Users\\Владислав\\Учеба\\Магистратура\\3 семестр\\synthetic data generation\\project\\utils\\QuantGAN_gaussianize.py:171: UserWarning: Warning: No convergence after 100 iterations. Increase max_iter.\n",
      "  warnings.warn(\"Warning: No convergence after %d iterations. Increase max_iter.\" % max_iter)\n",
      "c:\\Users\\Владислав\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\utils\\extmath.py:985: RuntimeWarning: invalid value encountered in divide\n",
      "  updated_mean = (last_sum + new_sum) / updated_sample_count\n",
      "c:\\Users\\Владислав\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\utils\\extmath.py:990: RuntimeWarning: invalid value encountered in divide\n",
      "  T = new_sum / new_sample_count\n",
      "c:\\Users\\Владислав\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\utils\\extmath.py:1020: RuntimeWarning: invalid value encountered in divide\n",
      "  new_unnormalized_variance -= correction ** 2 / new_sample_count\n",
      "Discriminator Loss: nan Generator Loss: nan: 100%|██████████| 4/4 [00:16<00:00,  4.19s/it]\n",
      "C:\\Users\\Владислав\\AppData\\Local\\Temp/ipykernel_10980/4122270993.py:64: RuntimeWarning: Mean of empty slice.\n",
      "  y -= y.mean()\n",
      "c:\\Users\\Владислав\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\numpy\\core\\_methods.py:190: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "C:\\Users\\Владислав\\AppData\\Local\\Temp/ipykernel_10980/4122270993.py:64: RuntimeWarning: Mean of empty slice.\n",
      "  y -= y.mean()\n",
      "c:\\Users\\Владислав\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\numpy\\core\\_methods.py:190: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "C:\\Users\\Владислав\\AppData\\Local\\Temp/ipykernel_10980/4122270993.py:64: RuntimeWarning: Mean of empty slice.\n",
      "  y -= y.mean()\n",
      "c:\\Users\\Владислав\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\numpy\\core\\_methods.py:190: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "C:\\Users\\Владислав\\AppData\\Local\\Temp/ipykernel_10980/4122270993.py:64: RuntimeWarning: Mean of empty slice.\n",
      "  y -= y.mean()\n",
      "c:\\Users\\Владислав\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\numpy\\core\\_methods.py:190: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "C:\\Users\\Владислав\\AppData\\Local\\Temp/ipykernel_10980/4122270993.py:64: RuntimeWarning: Mean of empty slice.\n",
      "  y -= y.mean()\n",
      "c:\\Users\\Владислав\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\numpy\\core\\_methods.py:190: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "C:\\Users\\Владислав\\AppData\\Local\\Temp/ipykernel_10980/4122270993.py:64: RuntimeWarning: Mean of empty slice.\n",
      "  y -= y.mean()\n",
      "c:\\Users\\Владислав\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\numpy\\core\\_methods.py:190: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "C:\\Users\\Владислав\\AppData\\Local\\Temp/ipykernel_10980/4122270993.py:64: RuntimeWarning: Mean of empty slice.\n",
      "  y -= y.mean()\n",
      "c:\\Users\\Владислав\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\numpy\\core\\_methods.py:190: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "C:\\Users\\Владислав\\AppData\\Local\\Temp/ipykernel_10980/4122270993.py:64: RuntimeWarning: Mean of empty slice.\n",
      "  y -= y.mean()\n",
      "c:\\Users\\Владислав\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\numpy\\core\\_methods.py:190: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "C:\\Users\\Владислав\\AppData\\Local\\Temp/ipykernel_10980/4122270993.py:64: RuntimeWarning: Mean of empty slice.\n",
      "  y -= y.mean()\n",
      "c:\\Users\\Владислав\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\numpy\\core\\_methods.py:190: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "C:\\Users\\Владислав\\AppData\\Local\\Temp/ipykernel_10980/4122270993.py:64: RuntimeWarning: Mean of empty slice.\n",
      "  y -= y.mean()\n",
      "c:\\Users\\Владислав\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\numpy\\core\\_methods.py:190: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 4\n",
    "ts_iterator = get_passengers_dataset(passengers_dataset_path, max_results=99)\n",
    "synthetic_path = f\"{passengers_dataset_path}synthetic/QuantGAN/\"\n",
    "start_point = 85\n",
    "for _ in range(start_point): next(ts_iterator)\n",
    "\n",
    "for ts_index, time_series in enumerate(ts_iterator, start_point):\n",
    "    print(f\"Time Series #{ts_index}\")\n",
    "    model_routine(time_series + 1e-9, ts_index, synthetic_path)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.special import kl_div\n",
    "\n",
    "from utils.data import get_hsm_dataset, get_solar_energy_dataset, get_fuel_prices_dataset, get_passengers_dataset, split_data, log_returns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_dir = Path(\"results\")\n",
    "\n",
    "seq_len = 127\n",
    "\n",
    "sj_div = lambda x, y: (kl_div(x, (x + y) / 2) + kl_div(y, (x + y) / 2)) / 2\n",
    "min_max_norm = lambda x: (x - x.min()) / (x.max() - x.min())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing ap dataset\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 800/800 [00:00<00:00, 20053.33it/s]\n",
      "100%|██████████| 787/787 [00:00<00:00, 20765.97it/s]\n",
      "100%|██████████| 796/796 [00:00<00:00, 15962.26it/s]\n",
      "100%|██████████| 586/586 [00:00<00:00, 20983.69it/s]\n",
      "100%|██████████| 797/797 [00:00<00:00, 18584.35it/s]\n",
      "100%|██████████| 787/787 [00:00<00:00, 21920.04it/s]\n",
      "100%|██████████| 732/732 [00:00<00:00, 22241.92it/s]\n",
      "100%|██████████| 664/664 [00:00<00:00, 20175.29it/s]\n",
      "100%|██████████| 776/776 [00:00<00:00, 22230.89it/s]\n",
      "100%|██████████| 769/769 [00:00<00:00, 19276.61it/s]\n",
      "100%|██████████| 743/743 [00:00<00:00, 20693.84it/s]\n",
      "100%|██████████| 799/799 [00:00<00:00, 21652.11it/s]\n",
      "100%|██████████| 784/784 [00:00<00:00, 19652.38it/s]\n",
      "14it [00:06,  2.31it/s]\n"
     ]
    }
   ],
   "source": [
    "start_dataset = 3\n",
    "start_ts = 85\n",
    "\n",
    "for ds_ind, (dataset_path, dataset_name) in enumerate(((Path(\"data/huge_stock_market_dataset/\"), \"hsm\"),\\\n",
    "     (Path(\"data/solar_energy\"), \"se\"), (Path(\"data/fuel_prices/\"), \"fp\"),\\\n",
    "        (Path(\"data/air_passengers/\"), \"ap\"))):\n",
    "    if ds_ind < start_dataset: continue\n",
    "    print(f\"processing {dataset_name} dataset\")\n",
    "    for model in (\"QuantGAN\",):\n",
    "        synthetic_path = dataset_path / f\"synthetic/{model}/\"\n",
    "        if (results_dir / f\"synth_{dataset_name}_sim_{model}.csv\").exists():\n",
    "            results = pd.read_csv(results_dir / f\"synth_{dataset_name}_sim_{model}.csv\").to_dict()\n",
    "        else:\n",
    "            results = {\"kl_div\": {}, \"sj_div\": {}}\n",
    "        if dataset_name == \"hsm\":\n",
    "            ts_iterator = get_hsm_dataset(dataset_path, selected_files=f\"{dataset_path}/selected100.csv\")\n",
    "        elif dataset_name == \"se\":\n",
    "            ts_iterator = get_solar_energy_dataset(dataset_path, max_results=10)\n",
    "        elif dataset_name == \"fp\":\n",
    "            ts_iterator = get_fuel_prices_dataset(dataset_path)\n",
    "        else:\n",
    "            ts_iterator = get_passengers_dataset(dataset_path, max_results=99)\n",
    "\n",
    "        for _ in range(start_ts): next(ts_iterator)\n",
    "\n",
    "        for ts_index, time_series in tqdm(enumerate(ts_iterator, start=start_ts)):\n",
    "            train_ts = log_returns(time_series + 1e-9).values.flatten()\n",
    "            train_ts = min_max_norm(train_ts)\n",
    "            train_tss = [train_ts[i: i + seq_len] for i in range(0, len(train_ts), seq_len) if i < len(train_ts) - seq_len + 1]\n",
    "            \n",
    "            synth_tss = np.load(synthetic_path / f\"selected{ts_index}.npy\")\n",
    "            if len(synth_tss) > 0:\n",
    "                kl_div_res = sj_div_res = 0\n",
    "                for synth_ts in tqdm(synth_tss):\n",
    "                    synth_ts = min_max_norm(synth_ts)\n",
    "                    # synth_ts = np.histogram(synth_ts, bins=np.arange(start=0, stop=1, step=1/100))[0]\n",
    "                    # train_ts = np.histogram(train_ts, bins=np.arange(start=0, stop=1, step=1/100))[0]\n",
    "                    for train_ts in train_tss:\n",
    "                        res = kl_div(synth_ts, train_ts)\n",
    "                        kl_div_res += np.where(np.isinf(res), 0, res).mean()\n",
    "                        sj_div_res += sj_div(synth_ts, train_ts).mean()\n",
    "                results[\"kl_div\"][ts_index] = kl_div_res / len(synth_tss) / len(train_tss)\n",
    "                results[\"sj_div\"][ts_index] = sj_div_res / len(synth_tss) / len(train_tss)\n",
    "            else:\n",
    "                results[\"kl_div\"][ts_index] = results[\"sj_div\"][ts_index] = 1\n",
    "        \n",
    "            pd.DataFrame(results).to_csv(results_dir / f\"synth_{dataset_name}_sim_{model}.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.9 64-bit (system)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "1289e797c8b2364a1b561fc46768e8fcf8446b2e18e77ab0795c8743ff6ac10a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
