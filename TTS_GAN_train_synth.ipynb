{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "# from torch.utils.tensorboard import SummaryWriter\n",
    "from pytorch_lightning import seed_everything\n",
    "\n",
    "from utils.data import get_hsm_dataset, get_solar_energy_dataset, split_data, log_returns\n",
    "from utils.metrics import MAPE, WAPE, MAE\n",
    "from utils.TTS_GAN import TTS_GAN_Generator, TTS_GAN_Discriminator, weights_init, train_TTS_GAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "hsm_dataset_path = \"data/huge_stock_market_dataset/\"\n",
    "solar_energy_dataset_path = \"data/solar_energy/\"\n",
    "models_dir = \"models/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "device = gpu = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)\n",
    "\n",
    "lr = 2e-4\n",
    "wd = 0\n",
    "ctrl_lr = 3.5e-4\n",
    "beta1 = 0.0\n",
    "beta2 = 0.9\n",
    "max_epoch = 20\n",
    "latent_dim = 128\n",
    "batch_size = gen_batch_size = dis_batch_size = 64\n",
    "ema = 0.995\n",
    "ema_kimg = 500\n",
    "ema_warmup = 0\n",
    "world_size = 0\n",
    "rank = - 1\n",
    "print_freq = 50\n",
    "n_critic = 1\n",
    "phi = 1\n",
    "accumulated_times = g_accumulated_times = 1\n",
    "loss = \"standard\"\n",
    "seq_len = 150\n",
    "\n",
    "n_samples = 1600 * 127  # number of samples generated by QuantGAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time Series #0\n",
      "generator loss: -0.6032 discriminator loss:  1.4732\n",
      "Time Series #1\n",
      "generator loss: -0.5496 discriminator loss:  1.3828\n",
      "Time Series #2\n",
      "generator loss: -0.5574 discriminator loss:  1.1996\n",
      "Time Series #3\n",
      "generator loss: -0.6008 discriminator loss:  1.4814\n",
      "Time Series #4\n",
      "generator loss: -0.3154 discriminator loss:  0.8688\n",
      "Time Series #5\n",
      "generator loss: -0.5753 discriminator loss:  1.3119\n",
      "Time Series #6\n",
      "generator loss: -0.5630 discriminator loss:  1.2776\n",
      "Time Series #7\n",
      "generator loss: -0.5748 discriminator loss:  1.3138\n",
      "Time Series #8\n",
      "generator loss: -0.5272 discriminator loss:  1.1397\n",
      "Time Series #9\n",
      "generator loss: -0.5831 discriminator loss:  1.3381\n",
      "Time Series #10\n",
      "generator loss: -0.5642 discriminator loss:  1.3654\n",
      "Time Series #11\n",
      "generator loss: -0.7545 discriminator loss:  1.9704\n",
      "Time Series #12\n",
      "generator loss: -0.5646 discriminator loss:  1.2984\n",
      "Time Series #13\n",
      "generator loss: -0.5375 discriminator loss:  1.1957\n",
      "Time Series #14\n",
      "generator loss: -0.5787 discriminator loss:  1.5024\n",
      "Time Series #15\n",
      "generator loss: -0.4910 discriminator loss:  1.1113\n",
      "Time Series #16\n",
      "generator loss: -0.4747 discriminator loss:  1.2222\n",
      "Time Series #17\n",
      "generator loss: -0.5686 discriminator loss:  1.2453\n",
      "Time Series #18\n",
      "generator loss: -0.6165 discriminator loss:  1.4918\n",
      "Time Series #19\n",
      "generator loss: -0.5980 discriminator loss:  1.5254\n",
      "Time Series #20\n",
      "generator loss: -0.7196 discriminator loss:  1.6859\n",
      "Time Series #21\n",
      "generator loss: -0.5759 discriminator loss:  1.3199\n",
      "Time Series #22\n",
      "generator loss: -0.5792 discriminator loss:  1.3208\n",
      "Time Series #23\n",
      "generator loss: -0.7363 discriminator loss:  1.7756\n",
      "Time Series #24\n",
      "generator loss: -0.5918 discriminator loss:  1.4304\n",
      "Time Series #25\n",
      "generator loss: -0.5476 discriminator loss:  1.2473\n",
      "Time Series #26\n",
      "generator loss: -0.6568 discriminator loss:  1.4975\n",
      "Time Series #27\n",
      "generator loss: -0.4625 discriminator loss:  1.0452\n",
      "Time Series #28\n",
      "generator loss: -0.7467 discriminator loss:  1.8337\n",
      "Time Series #29\n",
      "generator loss: -0.7773 discriminator loss:  2.2072\n",
      "Time Series #30\n",
      "generator loss: -0.5351 discriminator loss:  1.2967\n",
      "Time Series #31\n",
      "generator loss: -0.5603 discriminator loss:  1.3539\n",
      "Time Series #32\n",
      "generator loss: -0.4343 discriminator loss:  1.0979\n",
      "Time Series #33\n",
      "generator loss: -0.6152 discriminator loss:  1.4330\n",
      "Time Series #34\n",
      "generator loss: -0.7014 discriminator loss:  1.8804\n",
      "Time Series #35\n",
      "generator loss: -0.6165 discriminator loss:  1.3786\n",
      "Time Series #36\n",
      "generator loss: -0.6695 discriminator loss:  1.6047\n",
      "Time Series #37\n",
      "generator loss: -0.7412 discriminator loss:  1.7954\n",
      "Time Series #38\n",
      "generator loss: -0.6745 discriminator loss:  1.6673\n",
      "Time Series #39\n",
      "generator loss: -0.5575 discriminator loss:  1.2901\n",
      "Time Series #40\n",
      "generator loss: -0.7510 discriminator loss:  1.8451\n",
      "Time Series #41\n",
      "generator loss: -0.5101 discriminator loss:  1.1670\n",
      "Time Series #42\n",
      "generator loss: -0.7149 discriminator loss:  1.7259\n",
      "Time Series #43\n",
      "generator loss: -0.6799 discriminator loss:  1.6457\n",
      "Time Series #44\n",
      "generator loss: -0.5909 discriminator loss:  1.3956\n",
      "Time Series #45\n",
      "generator loss: -0.6170 discriminator loss:  1.5405\n",
      "Time Series #46\n",
      "generator loss: -0.5153 discriminator loss:  1.2127\n",
      "Time Series #47\n",
      "generator loss: -0.5673 discriminator loss:  1.4059\n",
      "Time Series #48\n",
      "generator loss: -0.6241 discriminator loss:  1.4939\n",
      "Time Series #49\n",
      "generator loss: -0.6440 discriminator loss:  1.5336\n",
      "Time Series #50\n",
      "generator loss: -0.5699 discriminator loss:  1.2981\n",
      "Time Series #51\n",
      "generator loss: -0.7932 discriminator loss:  1.9642\n",
      "Time Series #52\n",
      "generator loss: -0.5735 discriminator loss:  1.3673\n",
      "Time Series #53\n",
      "generator loss: -0.5236 discriminator loss:  1.2345\n",
      "Time Series #54\n",
      "generator loss: -0.6744 discriminator loss:  1.6626\n",
      "Time Series #55\n",
      "generator loss: -0.5752 discriminator loss:  1.3749\n",
      "Time Series #56\n",
      "generator loss: -0.5540 discriminator loss:  1.2658\n",
      "Time Series #57\n",
      "generator loss: -0.7585 discriminator loss:  1.8693\n",
      "Time Series #58\n",
      "generator loss: -0.5046 discriminator loss:  1.2583\n",
      "Time Series #59\n",
      "generator loss: -0.5863 discriminator loss:  1.3456\n",
      "Time Series #60\n",
      "generator loss: -0.5736 discriminator loss:  1.2996\n",
      "Time Series #61\n",
      "generator loss: -0.6873 discriminator loss:  1.6223\n",
      "Time Series #62\n",
      "generator loss: -0.6440 discriminator loss:  1.7156\n",
      "Time Series #63\n",
      "generator loss: -0.7366 discriminator loss:  1.8377\n",
      "Time Series #64\n",
      "generator loss: -0.7429 discriminator loss:  1.8783\n",
      "Time Series #65\n",
      "generator loss: -0.7314 discriminator loss:  1.9286\n",
      "Time Series #66\n",
      "generator loss: -0.6280 discriminator loss:  1.4746\n",
      "Time Series #67\n",
      "generator loss: -0.5606 discriminator loss:  1.2696\n",
      "Time Series #68\n",
      "generator loss: -0.5660 discriminator loss:  1.3015\n",
      "Time Series #69\n",
      "generator loss: -0.5786 discriminator loss:  1.3361\n",
      "Time Series #70\n",
      "generator loss: -0.4816 discriminator loss:  1.0684\n",
      "Time Series #71\n",
      "generator loss: -0.4271 discriminator loss:  1.1141\n",
      "Time Series #72\n",
      "generator loss: -0.6398 discriminator loss:  1.4897\n",
      "Time Series #73\n",
      "generator loss: -0.5928 discriminator loss:  1.3552\n",
      "Time Series #74\n",
      "generator loss: -0.5136 discriminator loss:  1.1015\n",
      "Time Series #75\n",
      "generator loss: -0.7077 discriminator loss:  1.6758\n",
      "Time Series #76\n",
      "generator loss: -0.4973 discriminator loss:  1.2257\n",
      "Time Series #77\n",
      "generator loss: -0.5670 discriminator loss:  1.2702\n",
      "Time Series #78\n",
      "generator loss: -0.6391 discriminator loss:  1.5129\n",
      "Time Series #79\n",
      "generator loss: -0.3526 discriminator loss:  1.0158\n",
      "Time Series #80\n",
      "generator loss: -0.7091 discriminator loss:  1.7515\n",
      "Time Series #81\n",
      "generator loss: -0.5609 discriminator loss:  1.3406\n",
      "Time Series #82\n",
      "generator loss: -0.5259 discriminator loss:  1.1844\n",
      "Time Series #83\n",
      "generator loss: -0.6009 discriminator loss:  1.3962\n",
      "Time Series #84\n",
      "generator loss: -0.5847 discriminator loss:  1.3419\n",
      "Time Series #85\n",
      "generator loss: -0.6457 discriminator loss:  1.5770\n",
      "Time Series #86\n",
      "generator loss: -0.5394 discriminator loss:  1.2729\n",
      "Time Series #87\n",
      "generator loss: -0.6269 discriminator loss:  1.5084\n",
      "Time Series #88\n",
      "generator loss: -0.6813 discriminator loss:  1.7666\n",
      "Time Series #89\n",
      "generator loss: -0.6577 discriminator loss:  1.5329\n",
      "Time Series #90\n",
      "generator loss: -0.6666 discriminator loss:  1.5674\n",
      "Time Series #91\n",
      "generator loss: -0.4952 discriminator loss:  1.0050\n",
      "Time Series #92\n",
      "generator loss: -0.6137 discriminator loss:  1.5162\n",
      "Time Series #93\n",
      "generator loss: -0.6058 discriminator loss:  1.4602\n",
      "Time Series #94\n",
      "generator loss: -0.5682 discriminator loss:  1.2711\n",
      "Time Series #95\n",
      "generator loss: -0.5236 discriminator loss:  1.1940\n",
      "Time Series #96\n",
      "generator loss: -0.5959 discriminator loss:  1.4379\n",
      "Time Series #97\n",
      "generator loss: -0.5874 discriminator loss:  1.3863\n",
      "Time Series #98\n",
      "generator loss: -0.6767 discriminator loss:  1.5396\n",
      "Time Series #99\n",
      "generator loss: -0.5336 discriminator loss:  1.3200\n"
     ]
    }
   ],
   "source": [
    "ts_iterator = get_hsm_dataset(hsm_dataset_path, selected_files=f\"{hsm_dataset_path}/selected100.csv\")\n",
    "synthetic_path = f\"{hsm_dataset_path}synthetic/TTS_GAN/\"\n",
    "seed_everything(0)\n",
    "\n",
    "start_point = 0\n",
    "for _ in range(start_point): next(ts_iterator)\n",
    "\n",
    "for ts_index, time_series in enumerate(ts_iterator, start=start_point):\n",
    "    print(f\"Time Series #{ts_index}\")\n",
    "    \n",
    "    train_ts = log_returns(time_series)\n",
    "\n",
    "    # using sequences of seq_len to train model\n",
    "    train_ts = np.array([train_ts[i: i + seq_len] for i in range(len(train_ts) - seq_len)])\n",
    "    train_dl = torch.utils.data.DataLoader(torch.from_numpy(train_ts.reshape(- 1, 1, 1, seq_len)).to(device), batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    TTS_GAN_gen = TTS_GAN_Generator(seq_len=seq_len, channels=1, latent_dim=latent_dim, ).to(device)\n",
    "    TTS_GAN_dis = TTS_GAN_Discriminator(seq_length=seq_len, in_channels=1).to(device)\n",
    "\n",
    "    gen_optimizer = torch.optim.AdamW(filter(lambda p: p.requires_grad, TTS_GAN_gen.parameters()), lr)\n",
    "    dis_optimizer = torch.optim.AdamW(filter(lambda p: p.requires_grad, TTS_GAN_dis.parameters()), lr)\n",
    "    \n",
    "    for epoch in range(max_epoch):\n",
    "        losses = train_TTS_GAN(globals(), TTS_GAN_gen, TTS_GAN_dis, gen_optimizer, dis_optimizer, train_dl, epoch)\n",
    "    tqdm.write(f\"generator loss: {losses[0]: 0.4f} discriminator loss: {losses[1]: 0.4f}\")\n",
    "    del dis_optimizer, gen_optimizer, TTS_GAN_dis, train_dl\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    samples_to_gen = n_samples // seq_len\n",
    "    synth_data = []\n",
    "    with torch.no_grad():\n",
    "        for _ in range(samples_to_gen):\n",
    "            z = torch.cuda.FloatTensor(np.random.normal(0, 1, (1, latent_dim))).cuda(device, non_blocking=True)\n",
    "            synth_data.append(TTS_GAN_gen(z).cpu().numpy())\n",
    "            del z\n",
    "            torch.cuda.empty_cache()\n",
    "    np.save(synthetic_path + f\"selected{ts_index}.npy\", np.row_stack(synth_data))\n",
    "\n",
    "    del TTS_GAN_gen, synth_data\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Time: ~53 min"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time Series #0\n",
      "generator loss: -0.3176 discriminator loss:  0.7872\n",
      "Time Series #1\n",
      "generator loss: -0.3169 discriminator loss:  0.7768\n",
      "Time Series #2\n",
      "generator loss: -0.3166 discriminator loss:  0.7941\n",
      "Time Series #3\n",
      "generator loss: -0.3146 discriminator loss:  0.7864\n",
      "Time Series #4\n",
      "generator loss: -0.3151 discriminator loss:  0.7826\n",
      "Time Series #5\n",
      "generator loss: -0.3160 discriminator loss:  0.7852\n",
      "Time Series #6\n",
      "generator loss: -0.3162 discriminator loss:  0.7807\n",
      "Time Series #7\n",
      "generator loss: -0.3144 discriminator loss:  0.7878\n",
      "Time Series #8\n",
      "generator loss: -0.3152 discriminator loss:  0.7822\n",
      "Time Series #9\n",
      "generator loss: -0.3441 discriminator loss:  0.8143\n"
     ]
    }
   ],
   "source": [
    "ts_iterator = get_solar_energy_dataset(solar_energy_dataset_path, max_results=10)\n",
    "synthetic_path = f\"{solar_energy_dataset_path}synthetic/TTS_GAN/\"\n",
    "seed_everything(0)\n",
    "max_epoch = 4\n",
    "\n",
    "start_point = 0\n",
    "for _ in range(start_point): next(ts_iterator)\n",
    "\n",
    "for ts_index, time_series in enumerate(ts_iterator, start=start_point):\n",
    "    print(f\"Time Series #{ts_index}\")\n",
    "    \n",
    "    train_ts = log_returns(time_series + 1e-9)\n",
    "\n",
    "    # using sequences of seq_len to train model\n",
    "    train_ts = np.array([train_ts[i: i + seq_len] for i in range(len(train_ts) - seq_len)])\n",
    "    train_dl = torch.utils.data.DataLoader(torch.from_numpy(train_ts.reshape(- 1, 1, 1, seq_len)).to(device), batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    TTS_GAN_gen = TTS_GAN_Generator(seq_len=seq_len, channels=1, latent_dim=latent_dim, ).to(device)\n",
    "    TTS_GAN_dis = TTS_GAN_Discriminator(seq_length=seq_len, in_channels=1).to(device)\n",
    "\n",
    "    gen_optimizer = torch.optim.AdamW(filter(lambda p: p.requires_grad, TTS_GAN_gen.parameters()), lr)\n",
    "    dis_optimizer = torch.optim.AdamW(filter(lambda p: p.requires_grad, TTS_GAN_dis.parameters()), lr)\n",
    "    \n",
    "    for epoch in range(max_epoch):\n",
    "        losses = train_TTS_GAN(globals(), TTS_GAN_gen, TTS_GAN_dis, gen_optimizer, dis_optimizer, train_dl, epoch)\n",
    "    tqdm.write(f\"generator loss: {losses[0]: 0.4f} discriminator loss: {losses[1]: 0.4f}\")\n",
    "    del dis_optimizer, gen_optimizer, TTS_GAN_dis, train_dl\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    samples_to_gen = n_samples // seq_len\n",
    "    synth_data = []\n",
    "    with torch.no_grad():\n",
    "        for _ in range(samples_to_gen):\n",
    "            z = torch.cuda.FloatTensor(np.random.normal(0, 1, (1, latent_dim))).cuda(device, non_blocking=True)\n",
    "            synth_data.append(TTS_GAN_gen(z).cpu().numpy())\n",
    "            del z\n",
    "            torch.cuda.empty_cache()\n",
    "    np.save(synthetic_path + f\"selected{ts_index}.npy\", np.row_stack(synth_data))\n",
    "\n",
    "    del TTS_GAN_gen, synth_data\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.special import kl_div\n",
    "\n",
    "from utils.data import get_hsm_dataset, get_solar_energy_dataset, split_data, log_returns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "hsm_dataset_path = \"data/huge_stock_market_dataset/\"\n",
    "solar_energy_dataset_path = \"data/solar_energy/\"\n",
    "results_dir = Path(\"results\")\n",
    "\n",
    "seq_len = 150\n",
    "\n",
    "sj_div = lambda x, y: (kl_div(x, (x + y) / 2) + kl_div(y, (x + y) / 2)) / 2\n",
    "min_max_norm = lambda x: (x - x.min()) / (x.max() - x.min())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing hsm dataset\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100it [01:06,  1.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing se dataset\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10it [07:44, 46.42s/it]\n"
     ]
    }
   ],
   "source": [
    "start_dataset = 0\n",
    "start_ts = 0\n",
    "\n",
    "for dataset_path, dataset_name in ((Path(\"data/huge_stock_market_dataset/\"), \"hsm\"),\\\n",
    "     (Path(\"data/solar_energy\"), \"se\")):\n",
    "    if dataset_name == \"hsm\" and start_dataset == 1: continue\n",
    "    print(f\"processing {dataset_name} dataset\")\n",
    "\n",
    "    for model in (\"TTS_GAN\",):\n",
    "        synthetic_path = dataset_path / f\"synthetic/TTS_GAN/\"\n",
    "        if dataset_name == \"hsm\":\n",
    "            ts_iterator = get_hsm_dataset(dataset_path, selected_files=f\"{dataset_path}/selected100.csv\")\n",
    "        else:\n",
    "            ts_iterator = get_solar_energy_dataset(dataset_path, max_results=10)\n",
    "        for _ in range(start_ts): next(ts_iterator)\n",
    "        results = {\"kl_div\": [], \"sj_div\": []}\n",
    "\n",
    "        for ts_index, time_series in tqdm(enumerate(ts_iterator)):\n",
    "            train_ts = log_returns(time_series if dataset_name == \"hsm\" else time_series + 1e-9).values.flatten()\n",
    "            train_ts = min_max_norm(train_ts)\n",
    "            train_tss = [train_ts[i: i + seq_len] for i in range(0, len(train_ts), seq_len) if i < len(train_ts) - seq_len + 1]\n",
    "            \n",
    "            synth_tss = np.load(synthetic_path / f\"selected{ts_index}.npy\")\n",
    "            kl_div_res = sj_div_res = 0\n",
    "            for synth_ts in tqdm(synth_tss, leave=False):\n",
    "                synth_ts = min_max_norm(synth_ts)\n",
    "                for train_ts in train_tss:\n",
    "                    res = kl_div(synth_ts, train_ts)\n",
    "                    kl_div_res += np.where(np.isinf(res), 0, res).mean()\n",
    "                    sj_div_res += sj_div(synth_ts, train_ts).mean()\n",
    "            results[\"kl_div\"].append(kl_div_res / len(synth_tss) / len(train_tss))\n",
    "            results[\"sj_div\"].append(sj_div_res / len(synth_tss) / len(train_tss))\n",
    "        \n",
    "        pd.DataFrame(results).to_csv(results_dir / f\"synth_{dataset_name}_sim_{model}.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.9 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "1289e797c8b2364a1b561fc46768e8fcf8446b2e18e77ab0795c8743ff6ac10a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
